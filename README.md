# Data Engineering Intern Project

This project demonstrates a small data engineering workflow using Python and SQLite3.  
It involves collecting data from an API, storing it locally, processing and analyzing it through a Jupyter Notebook, and finalizing it into a reproducible Python script.

---

## ğŸ“‚ Project Structure

â”œâ”€â”€ raw_data/ # Folder for raw files fetched from the API
â”œâ”€â”€ db/ # SQLite3 database files
â”œâ”€â”€ Working Notebook.ipynb # Jupyter notebook used for exploration and development
â”œâ”€â”€ final_project.py # Finalized, cleaned-up Python script
â””â”€â”€ README.md # Project documentation


---

## Technologies Used

- **Python 3**
- **SQLite3** â€“ lightweight local database for storing and querying data
- **Jupyter Notebook** â€“ for iterative coding, analysis, and visualization
- **Pandas / Requests/ Pydantic** â€“ for data manipulation and API integration

---

## Project Description

1. **Data Collection**  
   Data is fetched from an external API and stored as raw files in the `raw_data/` folder.  
   This ensures data persistence and allows reprocessing without repeated API calls.

2. **Database Layer**  
   The collected data is cleaned and stored in a local **SQLite3 database** under the `db/` directory (`stocks.db`).  
   SQLite is used for its simplicity and integration with Pythonâ€™s standard library.

3. **Development Notebook**  
   The `Working Notebook.ipynb` file contains exploratory work â€” initial data loading, transformations, visualizations, and logic testing.

4. **Final Code**  
   Once stable, all logic is consolidated into `final_project.py`, a reproducible, self-contained Python script.

---

## ğŸ—ƒï¸ Database

- Database Engine: **SQLite3**
- Location: `db/stocks.db`

---

## ğŸ§© How to Run

```bash
# Clone the repository
git clone https://github.com/datageek06/Data-Engineering-Intern-Project-.git
cd Data-Engineering-Intern-Project-

# Run the final script
python final_project.py
